from aws_link import upload_to_aws
from main import set_environment_variables
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.tools.tavily_search import TavilySearchResults
from tools import generate_image, generate_image_flux, get_data, RAG_TOOL, SQL_RAG
from typing import Annotated, Literal, TypedDict
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import END, START, StateGraph, MessagesState
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import AnyMessage, add_messages
from langchain_core.runnables import Runnable, RunnableConfig
import streamlit as st
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import tools_condition
from sqlconnector import open_connection, insert_data, delete_all, delete_table, close_connection
import os

from tools.image_gen_flux import api_key


class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def __call__(self, state: MessagesState, config: RunnableConfig):
        while True:

            state = {**state}
            result = self.runnable.invoke(state)
            # If the LLM happens to return an empty response, we will re-prompt it
            # for an actual response.
            if not result.tool_calls and (
                    not result.content
                    or isinstance(result.content, list)
                    and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}


set_environment_variables("WHaK AI")
# LLM = ChatOpenAI(model="gpt-3.5-turbo-0125")

SCRIPT_AGENT_NAME = "script_agent"
TAVILY_TOOL = TavilySearchResults(max_results=10, tavily_api_key="jIerUWieSJaYUrGSWZ6Fpxry8dftro2G")
tools = [TAVILY_TOOL, generate_image, generate_image_flux, get_data, SQL_RAG]
tool_node = ToolNode(tools)

primary_assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """"
You are a helpful and loyal agent with access to many tools.

If you need to pull information from a different conversation session, use the SQL_RAG tool
.
If you are asked to generate an image, use the generate_image tool. If you use this tool, be sure to only output the url generated by it and nothing else.


if you are asked how to do something, or information on something use your TAVILY_SEARCH_TOOL to search for urls related to the topic, please summarize the contents instead of returning the raw output.
        """
,
        ),
        ("placeholder", "{messages}"),
    ]
)


llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=1, api_key = os.environ["OPENAI_API_KEY"])


#model = primary_assistant_prompt | ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=1).bind_tools(tools)
model = primary_assistant_prompt | llm.bind_tools(tools)

# Define a new graph


workflow = StateGraph(MessagesState)

# Define the two nodes we will cycle between
workflow.add_node("agent", Assistant(model))
workflow.add_node("tools", tool_node)

# Set the entrypoint as `agent`
# This means that this node is the first one called
workflow.add_edge(START, "agent")

# We now add a conditional edge
workflow.add_conditional_edges(
    # First, we define the start node. We use `agent`.
    # This means these are the edges taken after the `agent` node is called.
    "agent",
    # Next, we pass in the function that will determine which node is called next.
    tools_condition,
)
workflow.add_edge("tools", "agent")

app = workflow.compile(checkpointer=MemorySaver())

def _print_event(event: dict, _printed: set, max_length=1500):
    current_state = event.get("dialog_state")
    if current_state:
        st.write("Currently in: ", current_state[-1])
    message = event.get("messages")
    if message:
        if isinstance(message, list):
            message = message[-1]
        if message.id not in _printed:
            msg_repr = message.pretty_repr(html=True)
            if len(msg_repr) > max_length:
                msg_repr = msg_repr[:max_length] + " ... (truncated)"
            st.write(msg_repr)
            _printed.add(message.id)

def get_event(event: dict, _printed: set):
    current_state = event.get("dialog_state")
    if current_state:
        st.write("Currently in: ", current_state[-1])
    message = event.get("messages")
    if message:
        if isinstance(message, list):
            message = message[-1]
        if message.id not in _printed:
            msg_repr = message.pretty_repr(html=True)
            return msg_repr




human_template = ""
iter = 0

on = st.toggle("Save to database")

uploadFile = st.file_uploader("Upload File", accept_multiple_files=True)

while human_template != "STOP":
    iter += 1

    placeholder = st.empty()
    with placeholder.container():
        human_template = st.text_input("Enter some text", key=iter)

    #askAi = st.button("Submit", key=iter * random())
    if human_template:
        messages =[
        {"role": "user", "content": human_template},
        ]

        final_state = app.invoke(
            {"messages": messages},
            config={"configurable": {"thread_id": 42}},

        )

        _printed = set()

        if on:
            state_string = get_event(final_state, _printed)
            conn = open_connection()
            insert_data(conn, human_template, state_string)
            close_connection(conn)
            st.write("Successfully uploaded response to database!")


        _print_event(final_state, _printed)
    else:
        st.stop()



